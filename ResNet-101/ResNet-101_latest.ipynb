{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 9\n",
      "Classes: ['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Near-full', 'Random', 'Scratch', 'none']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_data_loaders(train_dir, val_dir, test_dir, batch_size=32):\n",
    "    # Define the image transformations with optional adjustments\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.Resize((256, 256)),  # Experiment with 256x256\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load datasets using ImageFolder\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_dataset.classes, len(train_dataset.classes)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, val_loader, test_loader, class_names, num_classes = get_data_loaders(\n",
    "        'data/train', 'data/validation', 'data/test'\n",
    "    )\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Classes: {class_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_161836\\2854400053.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(latest_checkpoint))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_epoch_12.pth.\n",
      "Resuming training from epoch 13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [18:00<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 0.0729\n",
      "Epoch [13/20], Train Loss: 0.0729, Validation Loss: 0.0807, Validation Accuracy: 97.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [14:42<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 0.0674\n",
      "Model saved after epoch 14.\n",
      "Epoch [14/20], Train Loss: 0.0674, Validation Loss: 0.0849, Validation Accuracy: 97.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [15:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 0.0610\n",
      "Epoch [15/20], Train Loss: 0.0610, Validation Loss: 0.0698, Validation Accuracy: 97.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [15:37<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 0.0566\n",
      "Model saved after epoch 16.\n",
      "Epoch [16/20], Train Loss: 0.0566, Validation Loss: 0.0692, Validation Accuracy: 97.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [14:34<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 0.0540\n",
      "Epoch [17/20], Train Loss: 0.0540, Validation Loss: 0.0709, Validation Accuracy: 97.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [14:43<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 0.0542\n",
      "Model saved after epoch 18.\n",
      "Epoch [18/20], Train Loss: 0.0542, Validation Loss: 0.0754, Validation Accuracy: 97.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [15:54<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 0.0499\n",
      "Epoch [19/20], Train Loss: 0.0499, Validation Loss: 0.0714, Validation Accuracy: 97.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1830/1830 [16:35<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.0461\n",
      "Model saved after epoch 20.\n",
      "Epoch [20/20], Train Loss: 0.0461, Validation Loss: 0.0731, Validation Accuracy: 97.63%\n",
      "Final model saved as final_model.pth\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (8,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 133\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Loss plot\u001b[39;00m\n\u001b[0;32m    132\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrain Loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    135\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Coding\\Setups and Extensions\\Python\\python 3.12\\setup\\Lib\\site-packages\\matplotlib\\pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding\\Setups and Extensions\\Python\\python 3.12\\setup\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32md:\\Coding\\Setups and Extensions\\Python\\python 3.12\\setup\\Lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding\\Setups and Extensions\\Python\\python 3.12\\setup\\Lib\\site-packages\\matplotlib\\axes\\_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (8,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from data_preprocessing import get_data_loaders  # Import your data loader function\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths to your dataset\n",
    "train_dir = \"data//train\"\n",
    "val_dir = \"data//validation\"\n",
    "test_dir = \"data//test\"\n",
    "\n",
    "# Load data loaders\n",
    "train_loader, val_loader, test_loader, class_names, num_classes = get_data_loaders(train_dir, val_dir, test_dir)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  # Check which device is being used\n",
    "\n",
    "# Load the pretrained ResNet50 model\n",
    "model = models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "\n",
    "# Modify the final layer for your number of classes\n",
    "num_classes = len(class_names)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Set the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Check for the latest checkpoint and set resume_epoch\n",
    "resume_epoch = 0\n",
    "latest_checkpoint = 'model_epoch_12.pth'  # Modify if necessary, or use a variable to store the latest checkpoint dynamically\n",
    "\n",
    "if os.path.exists(latest_checkpoint):\n",
    "    model.load_state_dict(torch.load(latest_checkpoint))\n",
    "    print(f\"Model loaded from {latest_checkpoint}.\")\n",
    "    resume_epoch = int(latest_checkpoint.split('_')[-1].split('.')[0])\n",
    "    print(f\"Resuming training from epoch {resume_epoch + 1}.\")\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Track losses and accuracy for visualization\n",
    "train_losses, val_losses = [], []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop starting from resume_epoch\n",
    "num_epochs = 20  # Adjust as necessary\n",
    "for epoch in range(resume_epoch, num_epochs):  # Start from resume_epoch\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Save model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Model saved after epoch {epoch + 1}.\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and print average validation loss and accuracy for the epoch\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "            break  # Exit the training loop\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'final_model.pth')\n",
    "print(\"Final model saved as final_model.pth\")\n",
    "\n",
    "# Plotting the results\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Train Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "all_preds=[]\n",
    "all_labels=[]\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {100 * test_correct / test_total:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the training and validation losses\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m epochs_ran \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_losses\u001b[49m)  \u001b[38;5;66;03m# Get the number of completed epochs\u001b[39;00m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs_ran \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), train_losses[:epochs_ran], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs_ran \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), val_losses[:epochs_ran], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the training and validation losses\n",
    "epochs_ran = len(train_losses)  # Get the number of completed epochs\n",
    "plt.plot(range(1, epochs_ran + 1), train_losses[:epochs_ran], label='Training Loss')\n",
    "plt.plot(range(1, epochs_ran + 1), val_losses[:epochs_ran], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Losses\")\n",
    "plt.show()\n",
    "\n",
    "# Plot validation accuracy if available\n",
    "if val_accuracies:\n",
    "    plt.plot(range(1, epochs_ran + 1), val_accuracies[:epochs_ran], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m all_labels\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Testing loop\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      6\u001b[0m test_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m test_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "all_preds=[]\n",
    "all_labels=[]\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {100 * test_correct / test_total:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
